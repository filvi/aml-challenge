# -*- coding: utf-8 -*-
"""deepLearningSol.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GTYH1xXdNlfd3_z65JPJwv3MeG9cT0s-
"""



# Commented out IPython magic to ensure Python compatibility.
# %reload_ext autoreload
# %autoreload 2
# %matplotlib inline

import pandas as pd
import numpy
import pickle
import numpy as np
import bitarray
from lshashpy3 import LSHash #A fast Python implementation of locality sensitive hashing with persistance support.
# from lshash3  import LSHash #A fast Python implementation of locality sensitive hashing with persistance support.

# python -c "import lshashpy3 as lshash; print(lshash.__version__);"
import lshashpy3 as lshash
# import lshash3  as lshash

from fastai.vision import *
# from fastai.callback.hook import *
from fastai.callbacks.hooks import *
import matplotlib.pyplot as plt
from PIL import Image
import tqdm
import os
pd.set_option('display.max_columns', 500)
"""Generally-speaking, a common and basic building block for implementing sublinear time algorithms are hash functions. A hash function is any function that maps input into data of fixed size (usually of lower dimension). The most famous example, which you might have encountered by simply downloading files off the internet, is that of checksum hashes. The idea behind them is to generate a “finger-print” — i.e, some number that is hopefully unique for a particular chunk of data — that can be used to verify that the data was not corrupted or tampered with when it was transferred from one place to another.

checksum hash: good for exact duplicate detetction
These hash functions were designed with this sole purpose in mind. This means that they are actually very sensitive to small changes in the input data; even a single bit that’s changed will completely change the hash value. While this is really what we need for exact duplicate detection (e.g, flagging when two files are really the same), it’s actually the opposite of what we need for near duplicate detection.
This is precisely what Locality Sensitive Hashing (LSH) attempts to address. As it’s name suggest, LSH depends on the spatiality of the data; in particular, data items that are similar in high-dimension will have a larger chance of receiving the same hash value. This is the goal; there are numerous algorithms that construct hash functions with this property. I will describe one approach, that is amazingly simple and demonstrates the incredibly surprising power of random projections (for another example, see the beautiful Johnson-Lindenstrauss lemma).
The basic idea is that we generate a hash (or signature) of size k using the following procedure: we generate k random hyperplanes; the i-th coordinate of the hash value for an item x is binary: it is equal to 1 if and only if x is above the i-th hyperplane.
"""



# drive.mount('/content/gdrive', force_remount=True)



# we define training dataset
training_path =os.path.join(".", "dataset", "training")
validation_path = os.path.join(".", 'dataset', 'validation')

# we define validation dataset
# gallery_path = os.path.join('new_dataset', 'validation', 'gallery')
# query_path = os.path.join('new_dataset', 'validation', 'query')

class Dataset(object):
    def __init__(self, data_path):
        self.data_path = data_path
        assert os.path.exists(self.data_path), 'Insert a valid path!'

        # get class list
        self.data_classes = os.listdir(self.data_path)

        # init mapping dict
        self.data_mapping = {}

        # populate mapping dict
        for c, c_name in enumerate(self.data_classes):
            temp_path = os.path.join(self.data_path, c_name)
            temp_images = os.listdir(temp_path)

            for i in temp_images:
                img_tmp = os.path.join(temp_path, i)

                if img_tmp.endswith('.jpg'):
                    if c_name == 'distractor':
                        self.data_mapping[img_tmp] = -1
                    else:
                        self.data_mapping[img_tmp] = int(c_name)

        print('Loaded {:d} from {:s} images'.format(len(self.data_mapping.keys()),
                                                    self.data_path))

    def get_data_paths(self):
        # returns a list of imgpaths and related classes
        images = []
        classes = []
        for img_path in self.data_mapping.keys():
            if img_path.endswith('.jpg'):
                images.append(img_path)
                classes.append(self.data_mapping[img_path])
        return images, np.array(classes)


    def num_classes(self):
        # returns number of classes of the dataset
        return len(self.data_classes)
training_dataset = Dataset(data_path=training_path)




"""**FASTAI** provides a complete image transformation library written from scratch in PyTorch. Although the main purpose of the library is data augmentation for use when training computer vision models, you can also use it for more general image transformation purposes. Before we get in to the detail of the full API, we'll look at a quick overview of the data augmentation pieces that you'll almost certainly need to use.

## Data augmentation
Data augmentation is perhaps the most important regularization technique when training a model for Computer Vision: instead of feeding the model with the same pictures every time, we do small random transformations (a bit of rotation, zoom, translation, etc...) that don't change what's inside the image (to the human eye) but do change its pixel values. Models trained with data augmentation will then generalize better.

To get a set of transforms with default values that work pretty well in a wide range of tasks, it's often easiest to use get_transforms. 
"""

tfms = get_transforms(
    do_flip=True, # do_flip: if True the image is randomly flipped 
    flip_vert=False, # random apply horizontal flips
    max_rotate=45, # random rotation between -max_rotate and max_rotate degrees is applied with probability p_affine
    max_lighting=0.5, # a random lightning and contrast change controlled by max_lighting is applied with probability p_lighting 
    max_zoom=1.2, # random zoom between 1. and max_zoom is applied with probability p_affine
    max_warp=0.3, # a random symmetric warp of magnitude  is applied with probability p_affine
    p_affine=0.75, 
    p_lighting=0.75)

data = ImageDataBunch.from_folder(training_path, train='train', valid='test', valid_pct=0.2, ds_tfms=get_transforms(), size=224)
print('Number of classes {0}'.format(data.c))
data.show_batch(rows=3, figsize=(10,6), hide_axis=False)
# data_training = (ImageList.from_folder(training_path)
#         .split_by_rand_pct(0.2)
#         .label_from_folder()
#         .transform(tfms=tfms, size=224)
#         .databunch(bs=64))


learn = cnn_learner(data, models.resnet34, pretrained=True, metrics=accuracy)

learn.fit_one_cycle(4)
learn.save('stage1')
learn.load('stage1')

interp = ClassificationInterpretation.from_learner(learn)
type(interp)
fastai.train.ClassificationInterpretation

interp.plot_top_losses(9, figsize=(15,11), heatmap=False)

learn.lr_find()
learn.recorder.plot()

## Unfreeing layer and finding ideal learning rate if I have a freezed layer I need to unfreeze it to update weights
learn.unfreeze()

learn.fit_one_cycle(2, slice(1e-5, 1e-2/5))

learn.save('stage2')


learner = learn.load('stage2')


class SaveFeatures():
    features=None
    def __init__(self, m): 
        self.hook = m.register_forward_hook(self.hook_fn)
        self.features = None
    def hook_fn(self, module, input, output): 
        out = output.detach().cpu().numpy()
        if isinstance(self.features, type(None)):
            self.features = out
        else:
            self.features = np.row_stack((self.features, out))
    def remove(self): 
        self.hook.remove()
        
sf = SaveFeatures(learn.model[1][5]) ## Output before the last FC layer



## By running this feature vectors would be saved in sf variable initated above
_= learn.get_preds(data.train_ds)
_= learn.get_preds(DatasetType.Valid)

#Converting in a dictionary of {img_path:featurevector}
img_path = [str(x) for x in (list(data.train_ds.items)+list(data.valid_ds.items))]
feature_dict = dict(zip(img_path,sf.features))



with open('feature_dict.p', 'wb') as f:
    pickle.dump(feature_dict, f)


# Using Locality Sensitive hashing to find near similar images
from tqdm.notebook import tqdm

## Loading Feature dictionary
feature_dict = pickle.load(open('feature_dict.p','rb'))

## Locality Sensitive Hashing
# params
k = 10 # hash size
L = 5  # number of tables
d = 512 # Dimension of Feature vector
lsh = LSHash(hash_size=k, input_dim=d, num_hashtables=L)

# LSH on all the images
for img_path, vec in tqdm(feature_dict.items()):
    lsh.index(vec.flatten(), extra_data=img_path)
#HBox(children=(IntProgress(value=0, max=9144), HTML(value='')))

## Exporting as pickle
pickle.dump(lsh, open('lsh.p', "wb"))


## Loading Feature dictionary
feature_dict = pickle.load(open('feature_dict.p','rb'))
lsh = pickle.load(open('lsh.p','rb'))

def get_similar_items(idx, feature_dict, lsh_variable, n_items=10):
    response = lsh_variable.query(feature_dict[list(feature_dict.keys())[idx]].flatten(), 
                     num_results=n_items+1, distance_func='hamming')
    

    k_img = []
    for i in range(1, columns*rows +1):
        if i<n_items+2:
            img = Image.open(response[i-1][0][1])
            head, tail = os.path.split(response[i-1][0][1])
            fig.add_subplot(rows, columns, i)
            plt.imshow(img)
            k_img.append(tail)
    return k_img

def get_similar_item_vect(vect, feature_dict, lsh_variable, n_items=5):
    response = lsh_variable.query(vect, 
                     num_results=n_items+1, distance_func='hamming')
    
    columns = 8
    rows = int(np.ceil(n_items+1/columns))
    fig=plt.figure(figsize=(4*rows, 6*rows))
    k_img = []
    for i in range(1, columns*rows +1):
        if i<n_items+2:
            img = Image.open(response[i-1][0][1])
            fig.add_subplot(rows, columns, i)
            plt.imshow(img)
            head, tail = os.path.split(response[i-1][0][1])
            k_img.append(tail)
    return k_img
    

vec_query = []

_ = learner.predict(open_image(query_path +'/18/ec50k_00180056.jpg'))
vec_query.append(sf.features[-1])

_ = learner.predict(open_image(query_path +'/12/ec50k_00120010.jpg'))
vec_query.append(sf.features[-1])
#similar_img = get_similar_item_vect(vect, feature_dict, lsh, 10)
## Exporting as pickle
#pickle.dump(feature_dict, open(training_path/'feature_dict.p', 'wb'))



query_img = ['/18/ec50k_00180056.jpg', '/12/ec50k_00120010.jpg']

res = dict()
group = dict()
group['groupname'] = "Roosters"

for val in query_img:
  _ = learner.predict(open_image(query_path + val))
  vec = (sf.features[-1])
  res[val] = similar_img = get_similar_item_vect(vect, feature_dict, lsh, 10)

group["images"] = res
group
