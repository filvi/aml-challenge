# -*- coding: utf-8 -*-
"""deepLearningSol.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GTYH1xXdNlfd3_z65JPJwv3MeG9cT0s-
"""



# Commented out IPython magic to ensure Python compatibility.
# %reload_ext autoreload
# %autoreload 2
# %matplotlib inline

import pandas as pd
import numpy
import pickle
import numpy as np
import bitarray
from lshashpy3 import LSHash #A fast Python implementation of locality sensitive hashing with persistance support.
# from lshash3  import LSHash #A fast Python implementation of locality sensitive hashing with persistance support.

# python -c "import lshashpy3 as lshash; print(lshash.__version__);"
import lshashpy3 as lshash
# import lshash3  as lshash

from fastai.vision import *
# from fastai.callback.hook import *
from fastai.callbacks.hooks import *
import matplotlib.pyplot as plt
from PIL import Image
import tqdm
import os
pd.set_option('display.max_columns', 500)
"""Generally-speaking, a common and basic building block for implementing sublinear time algorithms are hash functions. A hash function is any function that maps input into data of fixed size (usually of lower dimension). The most famous example, which you might have encountered by simply downloading files off the internet, is that of checksum hashes. The idea behind them is to generate a “finger-print” — i.e, some number that is hopefully unique for a particular chunk of data — that can be used to verify that the data was not corrupted or tampered with when it was transferred from one place to another.

checksum hash: good for exact duplicate detetction
These hash functions were designed with this sole purpose in mind. This means that they are actually very sensitive to small changes in the input data; even a single bit that’s changed will completely change the hash value. While this is really what we need for exact duplicate detection (e.g, flagging when two files are really the same), it’s actually the opposite of what we need for near duplicate detection.
This is precisely what Locality Sensitive Hashing (LSH) attempts to address. As it’s name suggest, LSH depends on the spatiality of the data; in particular, data items that are similar in high-dimension will have a larger chance of receiving the same hash value. This is the goal; there are numerous algorithms that construct hash functions with this property. I will describe one approach, that is amazingly simple and demonstrates the incredibly surprising power of random projections (for another example, see the beautiful Johnson-Lindenstrauss lemma).
The basic idea is that we generate a hash (or signature) of size k using the following procedure: we generate k random hyperplanes; the i-th coordinate of the hash value for an item x is binary: it is equal to 1 if and only if x is above the i-th hyperplane.
"""



# drive.mount('/content/gdrive', force_remount=True)



# we define training dataset
training_path =os.path.join(".", "new_dataset", "training")
validation_path = os.path.join(".", 'new_dataset', 'validation')
print(training_path)
print(validation_path)
# we define validation dataset
# gallery_path = os.path.join('new_dataset', 'validation', 'gallery')
# query_path = os.path.join('new_dataset', 'validation', 'query')



"""**FASTAI** provides a complete image transformation library written from scratch in PyTorch. Although the main purpose of the library is data augmentation for use when training computer vision models, you can also use it for more general image transformation purposes. Before we get in to the detail of the full API, we'll look at a quick overview of the data augmentation pieces that you'll almost certainly need to use.

## Data augmentation
Data augmentation is perhaps the most important regularization technique when training a model for Computer Vision: instead of feeding the model with the same pictures every time, we do small random transformations (a bit of rotation, zoom, translation, etc...) that don't change what's inside the image (to the human eye) but do change its pixel values. Models trained with data augmentation will then generalize better.

To get a set of transforms with default values that work pretty well in a wide range of tasks, it's often easiest to use get_transforms. 
"""

tfms = get_transforms(
    do_flip=False, # do_flip: if True the image is randomly flipped (default behavior)
    flip_vert=False, # flip_vert: limit the flips to horizontal flips (when False) or to horizontal and vertical flips as well as 90-degrees rotations (when True)
    max_rotate=0, # if not None, a random rotation between -max_rotate and max_rotate degrees is applied with probability p_affine
    max_lighting=0, # if not None, a random lightning and contrast change controlled by max_lighting is applied with probability p_lighting 
    max_zoom=1, # if not 1. or less, a random zoom between 1. and max_zoom is applied with probability p_affine
    max_warp=0 # if not None, a random symmetric warp of magnitude between -max_warp and maw_warp is applied with probability p_affine
)



# data_training = (ImageList.from_folder(training_path)
#         .split_by_rand_pct(0.2)
#         .label_from_folder()
#         .transform(tfms=tfms, size=224)
#         .databunch(bs=64))


data = ImageDataBunch.from_folder(training_path, validation_path, valid_pct=0.2, ds_tfms=get_transforms(), size=224)

# print('Number of classes {0}'.format(data_training.c))

data_validation = (ImageList.from_folder(validation_path)
        .label_from_folder()
        .transform(tfms=tfms, size=224)
        .databunch(bs=64))

## Show sample data
data.show_batch(rows=3, figsize=(10,6), hide_axis=False)

"""### cnn_learner
``` cnn_learner(dls, arch, normalize=True, n_out=None, pretrained=True, config=None, loss_func=None, opt_func=Adam, lr=0.001, splitter=None, cbs=None, metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85, 0.95), cut=None, n_in=3, init=kaiming_normal_, custom_head=None, concat_pool=True, lin_ftrs=None, ps=0.5, first_bn=True, bn_final=False, lin_first=False, y_range=None) ```

Build a convnet style learner from dls and arch

The model is built from arch using the number of final activations inferred from dls if possible (otherwise pass a value to n_out). It might be pretrained and the architecture is cut and split using the default metadata of the model architecture (this can be customized by passing a cut or a splitter).

If normalize and pretrained are True, this function adds a Normalization transform to the dls (if there is not already one) using the statistics of the pretrained model. That way, you won't ever forget to normalize your data in transfer learning.
"""

## Creating the model
learn = cnn_learner(data, models.resnet50, pretrained=True, metrics=accuracy)

## Fitting 3 epochs -> to increase only for test an Freezing layer 
learn.fit_one_cycle(3)

## Unfreeing layer and finding ideal learning rate if I have a freezed layer I need to unfreeze it to update weights
learn.unfreeze()

"""### Train Model
ResNet weights are already powerful enough to achieve high classification accuracy without needing a large number of epochs (we use only 3 in this case)

"""

## Fitting 2 epochs
learn.fit_one_cycle(3)

## Saving model weights
learn.save('stg2-rn34')

# hooks are used for saving intermediate computations
class SaveFeatures():
    features=None
    def __init__(self, m): 
        self.hook = m.register_forward_hook(self.hook_fn)
        self.features = None
    def hook_fn(self, module, input, output): 
        out = output.detach().cpu().numpy()
        if isinstance(self.features, type(None)):
            self.features = out
        else:
            self.features = np.row_stack((self.features, out))
    def remove(self): 
        self.hook.remove()
        
sf = SaveFeatures(learn.model[1][5]) ## Output before the last FC layer

"""Saving intermediate layer features can be extremely important in many cases. Obvious use cases include using cnn features as an input to another model, appending other features with the cnn features, training only the last few layers and freezing the previous layers etc.


"""

## By running this feature vectors would be saved in sf variable initated above
_= learn.get_preds(data.train_ds)
_= learn.get_preds(DatasetType.Valid)

#Converting in a dictionary of {img_path:featurevector}
img_path = [str(x) for x in (list(data_training.train_ds.items)+list(data.valid_ds.items))]
feature_dict = dict(zip(img_path,sf.features))



with open('feature_dict.p', 'wb') as f:
    pickle.dump(feature_dict, f)

## Exporting as pickle
#pickle.dump(feature_dict, open(training_path/'feature_dict.p', 'wb'))

"""here the documentation of LSHash
[link text](https://pypi.org/project/lshash/)

here Pickle [link text](https://docs.python.org/3/library/pickle.html)
"""

# Using Locality Sensitive hashing to find near similar images
from tqdm.notebook import tqdm

## Loading Feature dictionary
feature_dict = pickle.load(open('feature_dict.p','rb'))

## Locality Sensitive Hashing
# params
k = 10 # hash size
L = 5  # number of tables
d = 512 # Dimension of Feature vector
lsh = LSHash(hash_size=k, input_dim=d, num_hashtables=L)

# LSH on all the images
for img_path, vec in tqdm(feature_dict.items()):
    lsh.index(vec.flatten(), extra_data=img_path)
#HBox(children=(IntProgress(value=0, max=9144), HTML(value='')))

## Exporting as pickle
pickle.dump(lsh, open('lsh.p', "wb"))


## Loading Feature dictionary
feature_dict = pickle.load(open('feature_dict.p','rb'))
lsh = pickle.load(open('lsh.p','rb'))

def get_similar_items(idx, feature_dict, lsh_variable, n_items=10):
    response = lsh_variable.query(feature_dict[list(feature_dict.keys())[idx]].flatten(), 
                     num_results=n_items+1, distance_func='hamming')
    

    k_img = []
    for i in range(1, columns*rows +1):
        if i<n_items+2:
            img = Image.open(response[i-1][0][1])
            head, tail = os.path.split(response[i-1][0][1])
            fig.add_subplot(rows, columns, i)
            plt.imshow(img)
            k_img.append(tail)
    return k_img

similar_img = get_similar_items(5, feature_dict, lsh, 10)

from os import listdir
from os.path import isfile, join
query_files = [f for f in listdir(query_path) if isfile(join(query_path, f))]
len(query_files)



img_name = query_files[4]
path = query_path + img_name
img = Image.open(path)
plt.imshow(img)

_ = learner.predict(open_image(path))
vect = sf.features[-1]
similar_img = get_similar_items(vect, feature_dict, lsh, 10)

similar_img

group = dict()
group['groupname'] = "Roosters"

res = dict()
query_arr = [2, 6, 8]
for val in query_arr:
    similar_img = get_similar_item(0, feature_dict, lsh, 10)
    head, tail = os.path.split(img_path[val])
    res[tail] = similar_img

group["images"] = res
group

group

"""  def submit(results, url):
        res = json.dumps(results)
        response = requests.post(url, res)
        result = json.loads(response.text)
        print(f"accuracy is {result['results']}")


    url = "http://kamino.disi.unitn.it:3001/results/"
    submit(group, url) """